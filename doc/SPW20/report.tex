\PassOptionsToPackage{table,xcdraw}{xcolor}

%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,nonacm]{acmart} 
\pagestyle{plain} % removes running headers

\bibliographystyle{ACM-Reference-Format}
%\citestyle{acmauthoryear}   %% For author/year citations

\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}

\input{listings-coq.tex}
\usepackage{booktabs} 
\usepackage{subcaption}

\newcommand{\N}{\mathbb{N}}
\newcommand{\asnc}{\texttt{asn1c}}
 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{relsize}
\usepackage{xcolor}
\usepackage{color}
\usepackage{mathtools}
\usepackage{enumitem}

%\usepackage{multicol}

\lstset{
  basicstyle=\ttfamily\small,
  frame=tb, % draw a frame at the top and bottom of the code block
  tabsize=4, % tab space width
  showstringspaces=false, % don't mark spaces in strings
  numbers=left, % display line numbers on the left
  commentstyle=\color{green}, % comment color
  keywordstyle=\color{blue}, % keyword color
  stringstyle=\color{red}, % string color
  % identifierstyle=\color{grey},
}

\lstdefinelanguage{diff}{
    morecomment=[f][\color{diffstart}]{@@},
    morecomment=[f][\color{diffincl}]{+},
    morecomment=[f][\color{diffrem}]{-},
  }

\begin{document}

\title{Work-in-Proress: Formally-Verified ASN.1 Protocol C-language Stack}

\author{Nika Pona}
\affiliation{
  \institution{Digamma.ai}
}
\email{npona@digamma.ai}
\author{Vadim Zaliva}
\affiliation{
  \institution{Carnegie Mellon University}
  \department{ECE}
}
\email{vzaliva@cmu.edu}

\begin{abstract}
TODO
\end{abstract}

\maketitle

\tableofcontents

\section{Introduction}

\subsection{Background}

The pervasively deployed and widely adopted ASN.1 (``Abstract Syntax
Notation One'') \cite{ASN1Intro} joint standard of the International
Telecommunication Union (ITU-T) and the International Organization for
Standardization (ISO/IEC) provides an essential interface description
language for defining data structures for serialized and deserialized
cross-platform data exchange, broadly used in telecommunications,
computer networking, the Internet, and cryptography. ASN.1 is
vitally-relied upon by core aspects of the Internet infrastructure and
Internet applications such as telephony, enterprise computing,
utilities, finance, military, security, digitally-controlled
infrastructure, transportation, medical systems, and commercial cloud
computing.

A simple example of ASN.1 module defining the messages (data
structures) of dummy \textit{Foo} Protocol is shown in
Listing~\ref{lst:asnex}.

\begin{lstlisting}[language=C,label=lst:asnex,
  caption={ASN.1 example}]  
    FooProtocol DEFINITIONS ::= BEGIN
    FooQuestion ::= SEQUENCE {
      someNumber INTEGER,
      question IA5String
    }
    
    FooAnswer ::= SEQUENCE {
      someOtherNumber INTEGER,
      answer BOOLEAN
    }
\end{lstlisting}

A typical ASN.1 stack comprises of a \textit{compiler} which parses
ASN.1 syntax definitions as shown in the Listing~\ref{lst:asnex} and
produces either a source code of a specialized protocol encoder and
decoder for this data or a runtime data for a parametric protocol
encoder and decoder.

However, the ASN.1 standard \cite{ASN1Intro} is large and complex:
currently, it comprises twelve sub-standards spanning 862 pages
supplemented by additional pages of corrigenda\cite{ASN1Intro}. This opens
a door for many potential software bugs and malicious exploits.
  
\subsection{Motivation}

Proliferating embedded and user computing devices are implementing
vast numbers of essential functions and applications, all of which
exchange data using ASN.1. The resulting interconnected communicating
systems are becoming riskier, less stable, less reliable, and
potentially dangerous. Disruption of the ASN.1 based communications
could threaten the functioning of the critical infrastructure our
society relies upon.


The Computer Vulnerabilities and Exposures (CVE) database
\cite{CVE} lists critical ASN.1-related bugs found each year in
the existing systems, and there have already been noteworthy exposures
\cite{OpenSSLMemoryCorruption} that although not as dire to security as first feared
\cite{ASN1Flaw} clearly spell out a first awareness of the vast risk and
exposure. We analyzed the last 4 years of ASN.1-related issues
reported in Computer Vulnerabilities and Exposures (CVE) database
\cite{CVEaddon}. Among vulnerabilities studied were CVEs for various
software and hardware products and vendors, including \textit{Apple},
\textit{axTLS}, \textit{Botan}, \textit{Bounty Castle},
\textit{librcrypto++}, \textit{libtasn}, \textit{LibTomCrypt},
\textit{Linux Kernel}, \textit{MatrixSSL}, \textit{Mozilla NSS
  (Firefox)}, \textit{Objective Systems}, \textit{OpenSSL},
\textit{PolarSSL}, \textit{RSA BSAFE}, \textit{Samba},
\textit{Samsung}, \textit{Snapdragon}, \textit{strongSwan}, and
\textit{Wireshark}. It was found that 39 out of 52 problems analyzed
were related to memory safety, 6 related to stack and heap bounds
checking, and 3 related to issues caused by applications accepting not
well-formed ASN.1 input. Having proved just six formal properties
would have prevented 49 out of 52 vulnerabilities, that is, more than
90\% of reported vulnerabilities.

\subsection{Our approach}

Systems and methods to date either (a) automatically tested but not
formally verified (b) use verification approach which rely on
automatic extraction from executable specifications (for example
involving network stack synthesis \cite{VNSSforSel4}, optimizing compilers
\cite{CompCert}, cryptographic libraries \cite{HACL}, and
encoder/decoders \cite{Narcissus}), or (c) apply a form of formal
verification which only proves partial correctness properties (partial
verification of NAT stack only proving parts of DPDK are specification
compliant \cite{NAT}, partial verification of Linux kernel TCP
implementation with 55\% line coverage and 92\% protocol coverage
\cite{NSDI}). Consequently, (a) and (c) do not provide sufficient
correctness guarantees, while (c) is often impractical due to poor
performance and compatibility limitations. In contrast, we pursue a
far deeper and comprehensive verification approach to performance and
portability and seek to prove actual industrial-level C-code
implementation.

As shown in Figure~\ref{fig:components} the project begins with ITU-T
standard document in the form of human-readable text. We will manually
convert it into formal specification (H.SPEC). This is high-level
specification (in Coq) which describes the correspondence between data
types (such as integers) and packet octets data layout. This
specification is one of the outputs of this project and has a value of
its own.

\begin{figure}[h!]
  \centering
  \includegraphics[width=10cm]{VerificationArchitectureDiagram.png}
  \caption{TODO:title}
  \label{fig:components}
\end{figure}

Next level of refinement is Executable Specification (E.SPEC). Also
written in Coq it describes ``encoder'' and ``decoder'' for each type
as a pair of pure functions. We will prove that the Executable
specification encodes and decodes bytes in conformance to High-Level
specification.

``High-Level Properties'' like termination, computation complexity and
memory safely could be proven based on executable specification. They
are shown as ``High Level'' box on the left. The executable spec
contains enough information to \textit{extract} a fully functional
encoder program in languages supported by Coq extraction mechanism
(e.g. OCaml) \cite{Extraction}. Such a possibility is shown with the box
on the left of Figure~\ref{fig:components} although we do not plan to
pursue it yet. This is approach others often take and it has some
drawbacks. However, it should be noted that all the work we will do up
to and including Executable Spec is compatible with this approach and
if needed we can revisit it. For example, in case if we need to
generate ASN.1 stack for special platforms where OCaml code opens some
additional opportunities for parallelization, optimization or
integration. One example of such platforms could be MirageOS
\cite{MirageOS} which is the OCaml-based unikernel.

The box at the left marked \textit{QuickChick} represents another
possibility we might not pursue immediately. It is using randomized
property-based automated testing based on on an executable
specification to further verify its correctness. This work could be
done using QuickChick \cite{QuickChick} in Coq. It is also could be used
to generate a test-suite for extracted code.

At the bottom of Figure 1 is a box labeled ``C'' which represents the
existing C code of ANS1C compiler. Using an automated conversion
provided by the clightgen tool within the CompCert C compiler
\cite{CompCert} we will convert this to an Abstract Syntax Tree (AST)
\cite{AST} .(labeled ``C.AST'' in Figure 1) rendered in a subset
of C-language called Clight \cite{Mechanized}. More specifically,
CompCert converts C ``concrete syntax'' to ``abstract'' Clight
syntax. The resulting Clight program is just a reification of the
original C program in Coq, retaining the overall structure and
preserving the semantics; Clight is further assigned formal semantics
by CompCert that can be used to further ``reason about'' (interrogate
and analyze) programs within Coq.

Finally, as the most critical (and laborious) step of the project, we
will prove that semantics of the semantics of AST of Clight
translation for each given function corresponds to executable
specification. This will guarantee that the C program behaves exactly
as the executable spec.

The formal certification of correctness will comprise the following
three elements.

\begin{enumerate}[label=(\alph*)]

\item Formal specification of ASN.1 subset, which can be examined.

\item Proofs of semantic equivalence between the C code and the
  specification (in Coq Proof Assistant \cite{Coq}). From such
  proofs, Coq can generate a ``certificate'' which is a program in a
  formal language based on the Calculus of Inductive Constructions
  (``CIC'') \cite{CIC}. The ``certificate'' allows 3rd party
  validation of proofs which have been developed in Coq. The Calculus
  of Inductive Constructions (``CIC'') is a small and mathematically
  well-defined formal language that serves as the underlying formal
  system of Coq. The generated certificate is automatically validated
  by the relatively small Coq kernel. Although a fraudulent
  certificate could hypothetically be created, it will not pass
  validation when submitted to the Coq kernel. The resulting outcome
  limits the trust need to trusting the small CIC formal language and
  the small Coq kernel.

\item Proofs of additional high-level properties such as memory safety
  and termination.

\end{enumerate}
  

Element (b) ensures a bug-free standard-compliant implementation, and
element (c) provides further guarantee that it could not be exploited
in penetration or denial of service attacks.

Additionally, our (modified) version of ASN1C could be compiled with
the certified CompCert compiler \cite{CompCert} to extend correctness
guarantees through all levels down to machine code. These project
results and product offerings will provide state-of-the-art high
assurance suitable for mission-critical systems. Further, since formal
verification will be layered atop of a widely used ASN.1 stack, it
could be immediately offered to current users. This will open new
markets by making it attractive to new users who require higher
assurance levels than current non-verified implementations provide.

\section{Preliminary work}

\subsection{Verifying floating-point numbers encoding}

As a first estimate of the difficluties associated with
verifying ASN.1-related programs, we wrote
formal specification of an encoder-decoder pair for a small, but
particularly error-prone subset of the standard - floating-point numbers \cite{ASN1Intro}.
Our first approach was relatively straight-forward: define types
representing ASN.1-encoded data in Coq, provide functions for converting
between representations and prove that they operate correctly.

As was clear from the start, this technique had major disadvantages.
First of all, our definitions, being written in pure Coq, had only one
connection to the real world - through automatic code extraction.
This immediately comes with a set of problems:

\begin{itemize}
\item Writing a new implementation in Coq does not produce any improvement in pre-existing widely-used implementations
\item Automatic code extraction does not provide sufficient correctness guarantees
\item Extracted code generally runs much slower than its counterparts implemented in other languages
\item Extracted code might not be compatible with or viable in some real-world use-case scenarios
\end{itemize}

However, this basic attempt allowed us to home in on what approaches
to verfication were viable and improve our effort estimates for the future. % and train one intern

\subsection{Verifying simple function}

To estimate an effort required to formally verify C code and to
experiment with various verification strategies we decided to try to
verify a small real function from existing ASN.1 compiler. We chose
function \texttt{asn\_strtoimax\_lim} from \texttt{asn1c} compiler.
The function is relatively simple, but at the same time uses many
features of C that make verifying imperative programs
challenging. \emph{XER} decoding functions for \emph{INTEGER},
\emph{OBJECT-IDENTIFIER} and \emph{RELATIVE-OID} types (and hence all
constructed types that use these primitive types) critically depend on
this function.

The only specification provided was the following comment in the
source code. Additional specification details have to be inferred from
the source code and usage examples.

\begin{quote}
 { \it Parse the number in the given string until the given *end position,
 returning the position after the last parsed character back using the
 same (*end) pointer.
 WARNING: This behavior is different from the standard strtol/strtoimax(3). }
\end{quote}

Full source code of the function is included in the Appendix~\ref{sec:stritomax}.

\subsubsection{Bugs}

Despite that fact that this function lineage could be traced back 15
years, and it being a part of mature, well-tested ASN.1 compiler used
many production systems during our formal verification exercise we
found three bugs in current implementation. These bugs were never
reported before and passed all human code reviews, unit, and fuzzying
tests. The following bugs were reported to product developers,
acknowledged and promptly fixed:
  
\paragraph{Negative range bug}

When we go beyond allowed \textit{int} range, a wrong result is given
for some inputs. For example, assuming we are working on a 8-bit
system and the maximum signed integer value is 127, parsing the input
string \texttt{``-1281''} produces the as a result
\emph{ASN\_STRTOX\_OK} with the value -127 instead of expected
\emph{ASN\_STRTOX\_ERROR\_RANGE}. This happens whenever the input
string represents a number smaller than \texttt{MIN\_INT}, due to the
fact that absolute value of \texttt{MIN\_INT} is greater than
\texttt{MAX\_INT}, thus negative number cannot be treated as
$\mathrm{value}\times\mathrm{sign}$ when $\mathrm{value}$ is
represented as \textit{int}. The bug was filed and promptly fixed by
developers:

\paragraph{Memory store bug}

Another bug we discovered was related to potentially overlapping
memory areas pointed by argument pointers. Under some circumstances
the value of the \texttt{end} pointer parameter is treated as a part
of the input data and the resulting error value could be
incorrect. This bug could never occur if the function is always called
with non-overlapping pointer arguments. However this could be viewed
as an implicit pre-condition which should be part of the function's
specification.

\paragraph{Specification Ambiguity}

After addressing the two bugs we discovered we were able to
successfully verify that the function finally corresponds to the
specification we wrote for. However, it was noticed the following
behavior: For input \texttt{``a''} it stores value 0 and returns {\color{green}\texttt{ASN\_STRTOX\_EXTRA\_DATA}} (same behaviour as on input \texttt{``0a''}), which could be unintended by authors. \\

  
\subsubsection{Direct operational semantics proof}

First we formulated functional correctness and proved it using
big-step operational semantics of \textit{C light}, defined in
\textit{Compcert}. In this proof we used the following pure-function
re-implementation in Coq: \texttt{asn\_strtoimax\_lim : addr $\rightarrow$
  addr $\rightarrow$ addr $\rightarrow$ option
  asn\_strtoimax\_lim\_result} as an intermediate specification.

This function took addresses as inputs and operated on memory using
\texttt{load} and \texttt{store} operations from \textit{CompCert's}
memory model, while calculating the resulting machine integer
value. The proof went by induction on the distance between input
pointers and the main difficulty apart trying to prove a faulty
program (that's when we discovered two bugs) was operational semantics
control flow minuities and machine arithmetic proofs. However, only a
couple of lemmas about the spec were needed. However, since functional
and memory specification were intertangled it was more difficult to
read the spec and make sure it was correct.

\subsubsection{Proof using VST}

The \textit{Verified Software Toolchain}\cite{VST} offered a
solutions to problems we encountered while doing direct opertational
semantics proof: it has good automation of control flow, some
automation for machine arithmetic, and clearly separates functional
and memory-related parts of the specification. It also provides a
uniform way of stating functional correctness and memory
safety. Proofs here are done in \textit{Hoare} and \textit{separation}
logic, which are proven to be sound with respect to operational
semantics. VST has tactics that can solve simple entailments in these
logics, however, they were not powerful enough to significantly reduce
the proof effort. In fact, with respect to memory specifications,
direct operational semantics proof was shorter and more
straightforward.

\paragraph{Specification layers} First we have a high-level specification of this function in declarative, relational style. Each constructor corresponds to a return message (state) and stores value the number of iterations of the function (used to store the result).

 \begin{lstlisting}[language=Coq]

(* Relation between input string, value, index an asn_strtox_result_e message *)
  Inductive asn_strtoimax_lim : list byte -> Z -> Z -> asn_strtox_result_e -> Prop :=
  (* Invalid data encountered *)
  | ERROR_INVAL:
      asn_strtoimax_lim nil 0 0 ERROR_INVAL
  (* More data expected (e.g. "+") *)
  | EXPECT_MORE : forall ls c,
      ls = [c]  ->
      is_sign c = true ->
      asn_strtoimax_lim ls 0 1 EXPECT_MORE
 (* Non-digit encountered *)
  | EXTRA_DATA : forall c ls z i,
      asn_strtoimax_lim ls z i OK ->
      is_digit c = false -> 
      asn_strtoimax_lim (ls ++ [c]) z i EXTRA_DATA
   ...    
  \end{lstlisting}

Next level is executable specification \texttt{Z\_of\_string} that is equivalent to the relational one, but which we can extract to Coq and that we use in the proof. We experimented with two approaches. In VST proof we used functional spec and used it in the proof of C function correctness directly.

 \begin{lstlisting}[language=Coq]
Fixpoint Z_of_string_loop (s : list byte) (v i : Z) (b : bool) := 
    match s with 
    | [] => {| res := OK; value := v; index := i |}
    | c :: tl => 
      if is_digit c
      then let v1 := app_char b v c in 
           if bounded v1
           then Z_of_string_loop tl v1 (i + 1) b
           else {| res := ERROR_RANGE; value := v1; index := i; |}      
      else {| res := EXTRA_DATA; value := v; index := i; |}              
    end.

Definition app_char (b : bool) v c := 
  if b then v * 10 + (Z_of_char c) 
       else v * 10 - (Z_of_char c).
 \end{lstlisting}

This required many lemmas to connect functional spec to C code and complicated the loop invariant. Given our experience with operational semantics proof, it seems more effective to separate functional aspect of the proof from C-proof to allow for more automation in the proof related to C and then prove the C-like function correctness as a typical functional correctness proof, which was simpler to do than prove lemmas.

 \begin{lstlisting}[language=Coq]
 
Fixpoint Z_of_string_loop_C (s : list byte) (v i : Z) (b : bool) := 
    match s with 
    | [] => {| res := OK ; value := v ; index := i |}
    | c :: tl => 
      if is_digit c
      then let d := (Z_of_char c) in 
           let v1 := v*10 + d in
           if v <? upper_boundary 
           then Z_of_string_loop_C tl v1 (i + 1) b
           else if (v =? upper_boundary)&&(d <=? (last_digit_max b))
                then match tl with
                     | [] => {| res := OK ; 
                               value := v1 ; 
                               index := (i + 1) |}
                     | c :: tl => if is_digit c
                                 then {| res := ERROR_RANGE ;
                                         value := app_char b v1 c;
                                         index := (i + 1) ; |}
                                 else {| res := EXTRA_DATA;
                                         value := v1;
                                         index := (i + 1) ; |}
                     end
                else {| res := ERROR_RANGE ; value := v1 ; index := i ; |}      
      else {| res := EXTRA_DATA ; value := v ; index := i ; |}              
    end.
    
 \end{lstlisting}

Then we can state VST specification using either high-level specification or one of executable specs. For instance the memory precondition for the function will now has the following form. [TODO: EXPLAIN VST SPEC LANGUAGE]
\begin{lstlisting}[language=Coq]
SEP ((* str and end' are comparable, i.e. point within the same object *)
           valid_pointer (end'_b, str_ofs + (Zlength ls)) ;
           valid_pointer (end'_b, end'_ofs) ;
           valid_pointer (str_b, str_ofs) ;
           (* str points to ls with readable permission *)
           data_at (tarray tschar (Zlength ls)) 
                   (map Vbyte ls) (str_b, str_ofs) ; 
           (* end points to end' *)
           data_at (tptr tschar) (end'_b, end'_ofs) 
                   (end_b, end_ofs);
           (* intp points to some value v  *)
           data_at (tlong) v (intp_b, intp_ofs))
           \end{lstlisting}

And the postcondition:
           
\begin{lstlisting}[language=Coq]
 SEP((* this part didn't change after execution *)
           valid_pointer (end'_b, str_ofs + (Zlength ls)) ;
           valid_pointer (end'_b, end'_ofs) ;
           valid_pointer (str_b, str_ofs) ;
           data_at (tarray tschar (Zlength ls)) 
                   (map Vbyte ls) (str_b, str_ofs) ; 
           let r := res (Z_of_string ls) in
            (* in 3 cases intp stays unchanged,
              otherwise store the end value of Z_of_string *)
            match r with 
              | ERROR_RANGE 
              | ERROR_INVAL 
              | EXPECT_MORE => 
                data_at (tlong) v (intp_b, intp_ofs)
              | _ => data_at (tlong) (value (Z_of_string ls))
                         (intp_b, intp_ofs) 
            end ;
           (* if str >= end, end doesn't change, 
              otherwise store the address of the last char read 
              (before going out of range, reading extra data 
              or successfully terminating) *)
            let i := index (Z_of_string ls) in
            if str_ofs <?  end'_ofs
            then data_at (tptr tschar) 
                         (str_b, str_ofs + i) 
                         (end_b, end_ofs)
            else data_at (tptr tschar) (end'_b, end'_ofs) 
                         (end_b, end_ofs)).
\end{lstlisting}

Given experiments on this example we delineate a strategy for proving
\texttt{asn1c} correctness.

\section{Project Scope}

The International Telecommunications Union X.509 standard
\cite{TODO:16} defines the format of public key certificates used in
many cryptography, Internet protocols (including TLS/SSL, the basis
for the HTTPS protocol for secure browsing the web), certificate
revocation lists, certification path validation, electronic
signatures, and many other essential applications. X.509 is based on
ASN.1, using an important central particular subset (Distinguished
Encoding Rules, ``DER'') of ASN.1 \cite{TODO:37}.

To limit the scope of the initial stage of the project to while
exercising and showcasing all involved with the complete ASN.1
standard, we decided to focus the detailed verification this part
which could be then immediately used to implement verified X.509
stacks for use in production applications.
 
The X.509 focus provides a setting to answer essentially all questions
that must be answered to determine the technical feasibility of the
proposed concept of full ASN.1 stack verification. These questions
include the following points.

\begin{enumerate}
\item How ASN1C source code needs to be refactored to make it suitable for verification? In particular:
  \begin{enumerate}
  \item What features of general C-language that are not supported by CompCert's C language semantics need to be avoided?
  \item Are there any code organization changes which will aid structuring proofs (for example to make it easier to relate each function to a corresponding lemma).
  \item Are there any simplifications which could be done to ASN1C code by removing rarely used or obscure features that will make it easier to verify?
  \item Since ASN1C codebase dates back almost a decade, are there are any low-level manual optimizations that could be eliminated as combination of modern compilers and hardware handles them as well?
  \end{enumerate}
  Of these, we plan to make the necessary simplifications of ASN1C
  code required for the scope of the Phase I subset. This work will
  also inform us as to the type and scope of changes needed for the
  rest of the codebase to complete full ASN1C verification in later
  stages.
\item What high-level properties can be proven which will translate into additional code safety guarantees?
\item What safety guarantees will be ensured by our verification?
\item How should formalization of ASN.1 standard in Coq look - i.e. what is the balance between clarity and readability (so humans can examine it and satisfy themselves that it indeed corresponds to the standard) and comprehensiveness (so meaningful set of properties could be proven based on it)?
\item How many of the proving steps could be automated and how these automations speed up proof process? Can these be used to estimate effort required to prove the remainder of the ASN.1 stack?
\item How many of the proving methodologies and tools developed during
  Phase I could be re-used to prove other similar software and
  protocols, particularly those that can be products? Examples include
  the following.
  \begin{enumerate}
    \item Network protocol C-language implementations.
    \item Arbitrary C-language programs.
    \item Network protocols implementations in other languages.
    \item Arbitrary programs on other languages.
    \end{enumerate}
  \item Are Executable Specifications of encoders/decoders completely sufficient to “extract” a working skeleton of ASN.1 stack in OCaml? How much of additional “glue” code needed to be written to transform the result into working product? How do code size and the performance of such extracted stack compares to the original C-language implementation from ASN1C?  
\end{enumerate}

To address these questions determining the technical feasibility of
our proposed concept, the following key objectives of the project have
been defined and structured as follows.

\begin{enumerate}
\item Formalize the X.509 part of the ASN.1 standard.     
\item Refactor ASN1C code to make it suitable for verification.
\item Establish correctness of encoders/decoders for primitive types.
\item Establish correctness of encoders/decoders for constructed types.
\item Prove high-level properties.
\item Experiment with OCaml code extraction from Executable Specifications.
\item Establish metrics and evaluate code bases to estimate the effort required to prove the remainder of ASN.1 stack.
\item Produce the final ASN1C code and associated documentation in the form of commercial product that could be sold.
\end{enumerate}

We plan to use the technical results to immediately implement verified
X.509 stacks for use in production applications with Digamma.ai
customers.
 
\section{Project Status}

Now we are working on executable specification of the \texttt{asn1c} compiler, the intermediate level between the high-level specification and the C code. There are many ways of approaching this, but what we have to keep in mind is the future proof effort, both with respect to the code and the high-level specification. Thus our executable specification should correspond to the actual implementation, at the same time being abstract enough.

 \texttt{asn1c} compiler has modular structure, so we can proceed with verification in a modular way and exploit this structure in the proofs. From a high-level view, \texttt{asn1c} compiler consists of the library of primitive types decoders (such as INTEGER, BOOLEAN, FLOATING POINT etc). Each of these can be verified with respect to its ASN.1 specification. Our work on floating point number is an example of this. On top of this, we need to specify and prove memory safety for each primitive decoding/encoding pair.

 Further, \texttt{asn1c} compiler contains functions that decode/encode constructed types (such as SEQUENCE, CHOICE, SET etc). Then from a given ASN.1 type \texttt{asn1c} compiler produces an internal representation of the type, which basically specifies which existing decoders/encoders to apply and in which order, as well as the output/input C structures for the functions. In Coq we can formalize these internal representations as trees with nodes labelled by the type's tags and decoder/encoder types\footnote{For the sake of simplicity, let's consider only one constructed type SEQUENCE and only decode function.}.

 \begin{lstlisting}[language=Coq]
Inductive decoder_type := BOOLEAN | INTEGER | SEQUENCE | ...

Inductive TYPE_descriptor :=
  DEF { tags : list Z ;
        decoder : decoder_type;
        elements : list TYPE_descriptor 
      }.
 \end{lstlisting}

Then a constructed type decoder will traverse the tree and apply respective primitive decoders. In \texttt{asn1c} it is implemented as a recursive function that readily translates into a nested recursive function in Coq (decoders for other constructed types have similar structure).
 
 \begin{lstlisting}[language=Coq]
 Fixpoint decoder (X : TYPE_descriptor) (ls : list byte)  :=
    match ls with
    | [] =>  OK ls
    | [t] => MORE ls       
    | t :: l :: bs =>          
      if check_tag t (tags X)
        then
          match decoder X with 
          | SEQUENCE => (fix seq_decoder XS bs :=
                        match XS with
                        | [] => OK bs
                        | X :: XS =>
                          match decoder X bs with
                            | OK r => seq_decoder XS r 
                            | r => r
                          end
                        end) XS bs
          end
          | INTEGER => integer_decoder l bs
          | BOOLEAN => bool_decoder l bs
          | ...                                    
        else ERROR (l::bs)
        end.
 \end{lstlisting}

Given a particular ASN.1 type definition that translates into a \texttt{TYPE\_descriptor} the \texttt{decoder} function for that particular type corresponds to a transition system with states were primitive decoders are called. Hence one of the ways we could formalize the compiler is as a function that creates a transition system (alike finite state machine) from a given ASN.1 type definition or \texttt{TYPE\_descriptor} tree. Since run of the recursive \texttt{decoder} function corresponds to the transition system, we could use this model in the proof of implementation correctness.

The \texttt{decoder} function above could be also thought of in terms of sequence parser combinator known from monadic parser combinators tradition\cite{TODO}. Assume you have given decoders for primitive functions. Then you can add an operator \texttt{;; : decoder asn\_value-> decoder asn\_value -> asn\_value} that takes two decoders and applies them sequentially \texttt{p1;;p2}; similar for list of parsers. Then the compiler can be seen as producing the concrete combination of parsers given \texttt{TYPE\_descriptor}.   


\section{Future work: automation}


\subsection{Machine arithmetic}

The first bug is related to data type ranges and modulo integer
arithmetic. These sort of problems are fairly common and require
careful coding to be avoided. Formal verification enforces a strict
mathematical model of all computer arithmetic and invariably exposes
all such bugs. We use CompCert's \texttt{Integer} library that
provides theory of 8-, 16-, 32- and 64-bit integers and 32- and 64-bit
pointers. It has some basic lemmas, however, doing proofs by hand is
quite tedious, however automation can be achieved here for programs
that allow no overflow, since then one can use
\texttt{Micromega}
tactics for automatically solving comparisons on Z (if overflow is
allowed, it is an NP-complete problem and human input may be needed:
decidability of difference constraints for modular arithmetic (and
hence machine integer arithmetic) is NP-complete, by simple reduction
from 3-coloring\cite{PointerConstraintsNP}, whereas the same task for
full Z is linear in number of variables and constraints).


\subsection{Separation Logic}

The second problem was related to \textit{pointer aliasing}. These problems are not immediately obvious because C language does not allow us to enforce any memory aliasing restrictions (unlike, say Rust). In formal verification, there is a rigorous model to analyze such kind of problems called \textit{separation logic}. VST provides some automation related to this, however, it is not sufficient, but one can build custom libraries and if restricted to specific domain separation logic proofs can be fully automated. VST supports (undecidable) expressive dependently typed higher-order logic. The built-in tactics can solve simple goals, but much manual proof effort is required to transform the goals. The VST-floyd is based on databases of hints and separation logic lemmmas and tries to apply them in a smart way. We conjecture that the full force of separation logic is unnecessary in most cases and we could restrict ourselves to tractable fragments of separation logic, for which decision procedures has been already implemented within SMT solvers. The latter can be integrated into Coq proofs using SMTCoq\footnote{\url{https://smtcoq.github.io/}}.


\section{Related Work}


The project EverParse is the closest to what we do. However, they
don't verify existing compiler but build their own. They define their
own input language which accepts C-like type definitions, from which
specs, implementations of parsers and proofs of correctness are
generated automatically. This is possible because the produced parsers
are combintions of existing parsers that are proved correct. They
follow the tradition of parser combinators\footnote{Starting with
  primitive parser combinators fail, return[x] (output x),
  \texttt{read\_byte}, and monadic composition of two parsers
  and\_then. Further one can define combinators for parsing of pairs
  (*), mapping of function on parser results (synth), filter of parser
  results etc.}. For each combinator there are two implementations:
functional and C-like. C-like implementation is written in subset of
F* that models imperative language, called Low*. Functions written in
Low* operate on blocks and offsets and use machine integer
formalization. They rely on automatic extraction from Low* to C using
the tool Kremlin.

Narcissus also constructs correct binary parsers from a verified
library of combinators written in Coq. It generates functional
parsers. Building on Narcissus there is a verified compiler in Coq for
parsers and formatters described using Protocol Buffers.

 
Project Everest \cite{TODO:19} is our most direct thematic competition, and some
of the high-level Project Everest documentation makes passing mention
to ASN.1. The Project Everest stewards adopted a different approach
based on F* \cite{TODO:4}, and the Project Everest ASN.1 work appears at best
still in far distant plans.

It is also noted that Galois did some work on ASN.1 verification in
the past (circa 2012) \cite{TODO:11}. It appears that Galois abandoned \cite{TODO:12} the
goal of full ASN.1 verification that we pursue in our project with our
more pragmatic approach; Galois is now only exploring a limited subset
ASN.1 verification adequate for the “vehicle-to-vehicle” (V2V) market
\cite{TODO:13}, but that particular subset has limited broader applicability and
the Galois effort appears encumbered by aspects of an unsuccessful
approach. It is noted that although our goal is eventually to verify
all ASN.1, we decided to start with a different (X.509-related) subset
of ASN.1; this subset is reasonably small, but X.509 is so widely used
that our initial verified implementation of our chosen subset will
have a large volume and wide range of immediate commercial
applications.


\appendix
\clearpage
\section{\emph{asn\_strtoimax\_lim} source}
\label{sec:stritomax}

{\fontsize{8}{4}\selectfont  \lstinputlisting[language=C]{asn_strtoimax_lim_old.c}}


\bibliography{report}

\end{document}
